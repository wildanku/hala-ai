# Hala AI Service

A multi-layer AI pipeline service for generating personalized Islamic spiritual and productivity journeys.

## ğŸ—ï¸ Architecture

This service implements a 5-layer pipeline architecture:

```
User Input â†’ Layer 1 â†’ Layer 2 â†’ Layer 3 â†’ Layer 4 â†’ Layer 5 â†’ Response
              â”‚          â”‚          â”‚          â”‚          â”‚
              â–¼          â–¼          â–¼          â–¼          â–¼
          Sanitize   Semantic    Safety      RAG       LLM
                     Validate   Guardrails  Retrieval  Inference
```

### Project Structure

```
app/
â”œâ”€â”€ api/                    # API Layer (Routes/Controllers)
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ endpoints/      # Route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py   # Health check endpoints
â”‚   â”‚   â”‚   â””â”€â”€ journey.py  # Journey generation endpoints
â”‚   â”‚   â””â”€â”€ schemas/        # Pydantic request/response models
â”‚   â””â”€â”€ deps.py             # Dependency injection
â”œâ”€â”€ core/                   # Core configurations
â”‚   â”œâ”€â”€ config.py           # Settings & environment
â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚   â””â”€â”€ responses.py        # Standardized responses
â”œâ”€â”€ pipelines/              # Multi-layer pipeline (Layers 1-5)
â”‚   â”œâ”€â”€ base.py             # Abstract pipeline layer
â”‚   â”œâ”€â”€ layer1_sanitization.py
â”‚   â”œâ”€â”€ layer2_semantic.py
â”‚   â”œâ”€â”€ layer3_safety.py
â”‚   â”œâ”€â”€ layer4_rag.py
â”‚   â”œâ”€â”€ layer5_inference.py
â”‚   â””â”€â”€ orchestrator.py     # Pipeline coordinator
â”œâ”€â”€ providers/              # LLM Providers (Strategy Pattern)
â”‚   â”œâ”€â”€ base.py             # Abstract LLM provider
â”‚   â”œâ”€â”€ gemini.py           # Google Gemini
â”‚   â”œâ”€â”€ openai.py           # OpenAI GPT
â”‚   â”œâ”€â”€ ollama.py           # Local Ollama
â”‚   â””â”€â”€ factory.py          # Provider factory
â”œâ”€â”€ services/               # Business logic services
â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â””â”€â”€ knowledge_sync_service.py
â”œâ”€â”€ db/                     # Database layer
â”‚   â”œâ”€â”€ postgresql/         # PostgreSQL connection & models
â”‚   â””â”€â”€ vector/             # ChromaDB for RAG
â”œâ”€â”€ utils/                  # Utilities
â”‚   â””â”€â”€ logging.py
â””â”€â”€ main.py                 # FastAPI entry point
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11+
- PostgreSQL 15+
- Gemini API Key (or other LLM provider)

### Installation

1. **Clone and setup environment**

```bash
cd hala-ai
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Configure environment**

```bash
cp .env.example .env
# Edit .env with your settings
```

3. **Initialize database**

```bash
# Create PostgreSQL database
createdb hala_ai

# Run migrations (when implemented)
alembic upgrade head
```

4. **Run the service**

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

5. **Access the API**

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
- Health Check: http://localhost:8000/api/v1/health

## ğŸ“¡ API Endpoints

### Health

- `GET /api/v1/health` - Basic health check
- `GET /api/v1/health/detailed` - Detailed health with all dependencies
- `GET /api/v1/health/providers` - List LLM providers status

### Journey

- `POST /api/v1/journey/generate` - Generate a personalized journey
- `POST /api/v1/journey/validate` - Validate input without generating

### Example Request

```bash
curl -X POST http://localhost:8000/api/v1/journey/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Saya ingin meningkatkan kebiasaan sholat tahajud",
    "language": "id"
  }'
```

## ğŸ§  Pipeline Layers

### Layer 1: Sanitization

- Input length validation (10-500 chars)
- Prompt injection detection
- Profanity filtering

### Layer 2: Semantic Validation

- Uses Sentence-Transformers (all-MiniLM-L6-v2)
- Cosine similarity check against official scopes
- Threshold: 0.45

### Layer 3: Safety Guardrails

- Crisis/self-harm detection with help resources
- Violence detection
- Haram topics filtering

### Layer 4: RAG Retrieval

- ChromaDB vector search
- Retrieves Quran verses, Hadith, Hala strategies
- Top-K results (default: 5)

### Layer 5: LLM Inference

- Supports multiple providers (Gemini, OpenAI, Ollama)
- JSON response format
- 14-day journey generation

## ğŸ”Œ Adding New LLM Provider

1. Create new provider in `app/providers/`

```python
from app.providers.base import BaseLLMProvider

class MyProvider(BaseLLMProvider):
    @property
    def provider_name(self) -> str:
        return "my_provider"

    async def generate(self, ...): ...
    async def health_check(self) -> bool: ...
```

2. Register in factory

```python
# app/providers/factory.py
from app.providers.my_provider import MyProvider

class LLMProviderFactory:
    _providers = {
        "my_provider": MyProvider,
        ...
    }
```

## ğŸ§ª Testing

```bash
# Run tests
pytest

# With coverage
pytest --cov=app --cov-report=html
```

## ğŸ“ License

Proprietary - Hala Journal
